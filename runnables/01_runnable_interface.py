"""
üîß Runnable Interface - Core Fundamentals

Complete exploration of the Runnable protocol:
1. Core Methods - invoke(), batch(), stream() fundamentals
2. Async Execution - ainvoke(), abatch(), astream()
3. Configuration - Runtime config, metadata, tags
4. Input/Output Schemas - Type introspection
5. Method Chaining - Composing runnables
6. Custom Runnable - Building from scratch
7. Runnable Inspection - Debugging and introspection
8. Config Propagation - Passing config through chains

Master the foundation of LangChain!
"""

import os
import asyncio
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable, RunnableLambda, RunnableConfig
from langchain_core.pydantic_v1 import BaseModel
from typing import Dict, Any, List, Iterator
import time

# Load environment variables
load_dotenv()


def core_methods_example():
    """Example 1: Core Runnable methods - invoke, batch, stream"""
    
    print("=" * 70)
    print("üîß Example 1: Core Runnable Methods")
    print("=" * 70)
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.7,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    prompt = PromptTemplate.from_template("Write a haiku about {topic}")
    parser = StrOutputParser()
    
    # Create a simple chain
    chain = prompt | chat | parser
    
    print("\nüìå All LangChain components are Runnables!")
    print("   Models, prompts, chains, parsers - same interface")
    
    # Method 1: invoke() - Single input
    print("\n1Ô∏è‚É£ invoke() - Process single input:")
    result = chain.invoke({"topic": "mountains"})
    print(f"   Input: {{'topic': 'mountains'}}")
    print(f"   Output: {result}")
    
    # Method 2: batch() - Multiple inputs
    print("\n2Ô∏è‚É£ batch() - Process multiple inputs efficiently:")
    results = chain.batch([
        {"topic": "ocean"},
        {"topic": "forest"},
        {"topic": "desert"}
    ])
    
    print("   Inputs: ocean, forest, desert")
    for i, result in enumerate(results, 1):
        print(f"   Result {i}: {result[:40]}...")
    
    # Method 3: stream() - Streaming output
    print("\n3Ô∏è‚É£ stream() - Real-time streaming:")
    print("   Streaming haiku about 'sunset': ", end="", flush=True)
    for chunk in chain.stream({"topic": "sunset"}):
        print(chunk, end="", flush=True)
    print()
    
    print("\nüí° Key insight: Same interface (invoke/batch/stream) works")
    print("   for ALL runnables - models, chains, custom functions!")
    print()


def async_execution_example():
    """Example 2: Async execution methods"""
    
    print("=" * 70)
    print("‚ö° Example 2: Async Execution")
    print("=" * 70)
    
    async def async_demo():
        chat = ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=0.5,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        
        prompt = PromptTemplate.from_template("Translate to {language}: {text}")
        chain = prompt | chat | StrOutputParser()
        
        print("\nüöÄ Async methods for non-blocking I/O")
        
        # 1. ainvoke()
        print("\n1Ô∏è‚É£ ainvoke() - Async single execution:")
        result = await chain.ainvoke({
            "language": "Spanish",
            "text": "Hello, how are you?"
        })
        print(f"   Result: {result}")
        
        # 2. abatch()
        print("\n2Ô∏è‚É£ abatch() - Async batch processing:")
        start = time.time()
        results = await chain.abatch([
            {"language": "French", "text": "Good morning"},
            {"language": "German", "text": "Good night"},
            {"language": "Italian", "text": "Thank you"}
        ])
        duration = time.time() - start
        
        print(f"   Processed 3 translations in {duration:.2f}s")
        for i, result in enumerate(results, 1):
            print(f"   Translation {i}: {result}")
        
        # 3. astream()
        print("\n3Ô∏è‚É£ astream() - Async streaming:")
        print("   Streaming: ", end="", flush=True)
        async for chunk in chain.astream({
            "language": "Japanese",
            "text": "Beautiful day"
        }):
            print(chunk, end="", flush=True)
        print()
        
        print("\nüí° Async is essential for:")
        print("   ‚Ä¢ FastAPI / async web frameworks")
        print("   ‚Ä¢ Concurrent request handling")
        print("   ‚Ä¢ Better performance under load")
    
    # Run async function
    asyncio.run(async_demo())
    print()


def configuration_example():
    """Example 3: Runtime configuration"""
    
    print("=" * 70)
    print("‚öôÔ∏è Example 3: Runtime Configuration")
    print("=" * 70)
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.7,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    prompt = PromptTemplate.from_template("Explain {concept} in one sentence")
    chain = prompt | chat | StrOutputParser()
    
    print("\nüìã Config allows runtime customization and metadata")
    
    # Config with metadata
    config = {
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        },
        "tags": ["explanation", "educational"],
        "run_name": "concept_explainer"
    }
    
    print("\n1Ô∏è‚É£ Invoke with config:")
    print(f"   Config: {config}")
    
    result = chain.invoke(
        {"concept": "quantum computing"},
        config=config
    )
    
    print(f"   Result: {result}")
    
    print("\nüí° Config is useful for:")
    print("   ‚Ä¢ User tracking and analytics")
    print("   ‚Ä¢ A/B testing different prompts")
    print("   ‚Ä¢ Environment-specific behavior")
    print("   ‚Ä¢ Custom callbacks and monitoring")
    print()


def schema_introspection_example():
    """Example 4: Input/Output schema inspection"""
    
    print("=" * 70)
    print("üîç Example 4: Schema Introspection")
    print("=" * 70)
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.5,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    prompt = PromptTemplate(
        template="Summarize: {text}",
        input_variables=["text"]
    )
    
    chain = prompt | chat | StrOutputParser()
    
    print("\nüìä Every Runnable has input/output schemas")
    
    # Get schemas
    print("\n1Ô∏è‚É£ Input Schema:")
    input_schema = chain.input_schema.schema()
    print(f"   Type: {input_schema.get('type')}")
    print(f"   Properties: {list(input_schema.get('properties', {}).keys())}")
    
    print("\n2Ô∏è‚É£ Output Schema:")
    output_schema = chain.output_schema.schema()
    print(f"   Type: {output_schema.get('type')}")
    
    print("\n3Ô∏è‚É£ Individual component schemas:")
    print(f"   Prompt input: {prompt.input_schema.schema()['type']}")
    print(f"   Model output type: {type(chat.invoke('test')).__name__}")
    print(f"   Parser output: {parser.output_schema.schema()['type']}")
    
    print("\nüí° Schema introspection helps with:")
    print("   ‚Ä¢ Type validation")
    print("   ‚Ä¢ API documentation")
    print("   ‚Ä¢ IDE autocomplete")
    print("   ‚Ä¢ Runtime type checking")
    print()


def method_chaining_example():
    """Example 5: Method chaining and composition"""
    
    print("=" * 70)
    print("üîó Example 5: Method Chaining")
    print("=" * 70)
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.6,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    print("\nüéØ Runnables can be chained in multiple ways")
    
    # Way 1: Pipe operator
    print("\n1Ô∏è‚É£ Pipe operator (recommended):")
    chain1 = (
        PromptTemplate.from_template("Topic: {topic}")
        | chat
        | StrOutputParser()
        | RunnableLambda(lambda x: x.upper())
    )
    
    result1 = chain1.invoke({"topic": "AI"})
    print(f"   Result: {result1[:50]}...")
    
    # Way 2: with_ methods
    print("\n2Ô∏è‚É£ with_ methods for configuration:")
    
    # with_fallbacks
    backup_chain = PromptTemplate.from_template("Simple answer: {topic}") | chat
    chain_with_fallback = chain1.with_fallbacks([backup_chain])
    
    print("   ‚úì Added fallback chain")
    
    # with_retry
    chain_with_retry = chain1.with_retry(
        stop_after_attempt=3,
        wait_exponential_jitter=True
    )
    
    print("   ‚úì Added retry logic (3 attempts)")
    
    # with_config
    chain_with_config = chain1.with_config(
        tags=["production"],
        metadata={"version": "1.0"}
    )
    
    print("   ‚úì Bound default config")
    
    print("\nüí° Method chaining enables:")
    print("   ‚Ä¢ Clean, readable code")
    print("   ‚Ä¢ Composable error handling")
    print("   ‚Ä¢ Reusable configurations")
    print()


def custom_runnable_example():
    """Example 6: Creating custom Runnables"""
    
    print("=" * 70)
    print("üé® Example 6: Custom Runnable")
    print("=" * 70)
    
    class TextAnalyzer(Runnable):
        """Custom Runnable that analyzes text"""
        
        def invoke(self, input: Dict[str, Any], config: RunnableConfig = None) -> Dict[str, Any]:
            """Analyze text and return statistics"""
            text = input.get("text", "")
            
            return {
                "original": text,
                "word_count": len(text.split()),
                "char_count": len(text),
                "uppercase_count": sum(1 for c in text if c.isupper()),
                "sentence_count": text.count('.') + text.count('!') + text.count('?')
            }
        
        def batch(self, inputs: List[Dict[str, Any]], config: RunnableConfig = None) -> List[Dict[str, Any]]:
            """Batch analysis"""
            return [self.invoke(input, config) for input in inputs]
        
        def stream(self, input: Dict[str, Any], config: RunnableConfig = None) -> Iterator[Dict[str, Any]]:
            """Stream analysis results"""
            result = self.invoke(input, config)
            for key, value in result.items():
                yield {key: value}
    
    print("\nüõ†Ô∏è Custom Runnable: TextAnalyzer")
    
    analyzer = TextAnalyzer()
    
    # Test invoke
    print("\n1Ô∏è‚É£ invoke():")
    result = analyzer.invoke({
        "text": "Hello World! This is a test. How are you?"
    })
    
    for key, value in result.items():
        print(f"   {key}: {value}")
    
    # Test batch
    print("\n2Ô∏è‚É£ batch():")
    results = analyzer.batch([
        {"text": "Short text."},
        {"text": "A longer piece of text with more words."}
    ])
    
    for i, result in enumerate(results, 1):
        print(f"   Text {i}: {result['word_count']} words")
    
    # Test stream
    print("\n3Ô∏è‚É£ stream():")
    print("   Streaming analysis:")
    for chunk in analyzer.stream({"text": "Custom runnable test!"}):
        print(f"   {chunk}")
    
    # Compose with other runnables
    print("\n4Ô∏è‚É£ Composable with other runnables:")
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.5,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    # Analyze ‚Üí Generate summary
    combined = (
        analyzer
        | RunnableLambda(lambda x: {
            "analysis": f"Text has {x['word_count']} words and {x['char_count']} characters"
        })
        | PromptTemplate.from_template("Create a title for text with: {analysis}")
        | chat
        | StrOutputParser()
    )
    
    title = combined.invoke({"text": "Artificial intelligence is transforming technology."})
    print(f"   Generated title: {title}")
    
    print("\nüí° Custom Runnables allow:")
    print("   ‚Ä¢ Domain-specific logic")
    print("   ‚Ä¢ Reusable components")
    print("   ‚Ä¢ Full composition support")
    print()


def inspection_debugging_example():
    """Example 7: Runnable inspection and debugging"""
    
    print("=" * 70)
    print("üêõ Example 7: Inspection & Debugging")
    print("=" * 70)
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.5,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    # Build a chain
    chain = (
        PromptTemplate.from_template("Explain {topic}")
        | chat
        | StrOutputParser()
        | RunnableLambda(lambda x: x.upper())
    )
    
    print("\nüîç Inspecting chain structure")
    
    # 1. String representation
    print("\n1Ô∏è‚É£ Chain structure:")
    print(f"   {chain}")
    
    # 2. Get steps
    print("\n2Ô∏è‚É£ Pipeline steps:")
    print(f"   Type: {type(chain).__name__}")
    
    # 3. Add debug logging
    print("\n3Ô∏è‚É£ Adding debug logging:")
    
    def debug_log(x):
        print(f"   [DEBUG] Intermediate: {str(x)[:50]}...")
        return x
    
    debug_chain = (
        PromptTemplate.from_template("What is {topic}?")
        | RunnableLambda(debug_log)
        | chat
        | RunnableLambda(debug_log)
        | StrOutputParser()
        | RunnableLambda(debug_log)
    )
    
    print("   Running with debug logging:")
    result = debug_chain.invoke({"topic": "LCEL"})
    
    print("\nüí° Debugging techniques:")
    print("   ‚Ä¢ Insert RunnableLambda for logging")
    print("   ‚Ä¢ Inspect input/output schemas")
    print("   ‚Ä¢ Use callbacks for detailed tracing")
    print("   ‚Ä¢ Test components individually")
    print()


def config_propagation_example():
    """Example 8: Config propagation through chains"""
    
    print("=" * 70)
    print("üì° Example 8: Config Propagation")
    print("=" * 70)
    
    print("\nüîÑ Config automatically propagates through chains")
    
    # Custom runnable that uses config
    class ConfigAwareRunnable(Runnable):
        """Runnable that accesses config"""
        
        def invoke(self, input: Dict[str, Any], config: RunnableConfig = None) -> Dict[str, Any]:
            # Access config metadata
            metadata = config.get("metadata", {}) if config else {}
            user_id = metadata.get("user_id", "anonymous")
            
            return {
                **input,
                "processed_by": user_id,
                "timestamp": time.time()
            }
    
    chat = ChatGoogleGenerativeAI(
        model="gemini-pro",
        temperature=0.5,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    # Chain with config-aware component
    chain = (
        ConfigAwareRunnable()
        | RunnableLambda(lambda x: {
            "user": x["processed_by"],
            "text": f"User {x['processed_by']} asks: {x.get('question', '')}"
        })
        | PromptTemplate.from_template("Answer: {text}")
        | chat
        | StrOutputParser()
    )
    
    print("\n1Ô∏è‚É£ Without config:")
    result1 = chain.invoke({"question": "What is AI?"})
    print(f"   User: anonymous")
    print(f"   Result: {result1[:60]}...")
    
    print("\n2Ô∏è‚É£ With config:")
    config = {
        "metadata": {
            "user_id": "alice_123",
            "session": "prod_session"
        }
    }
    
    result2 = chain.invoke({"question": "What is ML?"}, config=config)
    print(f"   User: alice_123")
    print(f"   Result: {result2[:60]}...")
    
    print("\nüí° Config propagation enables:")
    print("   ‚Ä¢ User-specific behavior")
    print("   ‚Ä¢ Request tracing")
    print("   ‚Ä¢ A/B testing")
    print("   ‚Ä¢ Analytics and monitoring")
    print()


def main():
    """Run all Runnable interface examples"""
    
    print("\n" + "üîß" * 35)
    print("Welcome to Runnable Interface - Core Fundamentals!")
    print("üîß" * 35 + "\n")
    
    if not os.getenv("GOOGLE_API_KEY"):
        print("‚ùå Error: GOOGLE_API_KEY not found!")
        return
    
    try:
        core_methods_example()
        input("Press Enter to continue...")
        
        async_execution_example()
        input("Press Enter to continue...")
        
        configuration_example()
        input("Press Enter to continue...")
        
        schema_introspection_example()
        input("Press Enter to continue...")
        
        method_chaining_example()
        input("Press Enter to continue...")
        
        custom_runnable_example()
        input("Press Enter to continue...")
        
        inspection_debugging_example()
        input("Press Enter to continue...")
        
        config_propagation_example()
        
        print("=" * 70)
        print("‚úÖ Runnable Interface examples completed!")
        print("=" * 70)
        print("\nüéØ Key Takeaways:")
        print("  ‚úì invoke/batch/stream - Core execution methods")
        print("  ‚úì Async versions for non-blocking I/O")
        print("  ‚úì Config for metadata and customization")
        print("  ‚úì Schemas for type introspection")
        print("  ‚úì Method chaining for composition")
        print("  ‚úì Custom runnables for domain logic")
        print("  ‚úì Debugging with logging and inspection")
        print("  ‚úì Config propagation for context")
        print("\nüí° Next: RunnableSequence for sequential composition!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
